---
comments: false
title: متن اپیزود ششم ایستگاه هوش‌ مصنوعی
showToc: false
_build:
  list: false
---
  

توجه کنید که این متن توسط مدل‌های هوش‌مصنوعی تولید شده است و احتمال خطا در آن وجود دارد. البته این دفعه خیلی خطا داشت خودمم ویرایش کردم.
  

[0.00s -> 30.00s] سلام، من امیر پورمندم و این قسمت ششم ایستگاه هوش مصنوعیه. ما تو این پادکست راجع به هوش مصنوعی و هر چیزی که بهش ربط داره صحبت می کنیم. و این قسمت راجع به یادگیری عمیقه

[30.00s -> 60.00s] دیپ لرنینگ باعث چه اتفاقاتی شد؟ چرا هر جا میریم یکی داره یه شبکه عصبی دیپ رو برمی داره و با هاش یه کار جالب انجام میده این قدرت از کجا میاد؟ مسیری که تا حالا اومدیم برام مسیر جذابی بود و

[60.00s -> 90.00s] ما اول از هوش مصنوعی صحبت کردیم و گفتیم هوش مصنوعی چی هست و چی نیست چکار میتونه بکنه و چکار نمیتونه بکنه بعد به مدل ها رسیدیم و گفتیم که تو یادگیری ماشین بحث اینه که یه تابع رو بتونیم یاد بگیریم یعنی اگر بتونیم یه تابع رو یاد بگیریم و اون تابع رو خروجی بدیم مسئله مون هست

[90.00s -> 120.00s] مسئله ما اینه که بتونیم یه تابع پیدا کنیم. تو قسمت قبل هم یه خلاصه حالا از اون کارهایی که یه دیتا ساینتیس انجام میده گفتم و گفتم که یه دیتا ساینتیس مثلا از وقتی که شروع به کار میکنه از صبح تا شب کارش به چی میگذره. داره اون چی کار میکنه برا خودش. می خواستم که کم کم

[120.00s -> 150.00s] زودتر وارد بحث مدل های زبانی بزرگ بشم. چیزایی مثل ChatGPT و امثال هم ولی هرچی بهش فکر کردم دیدم که نمیشه. یعنی اصلا نمیشه راجع به هوش مصنوعی حرف زد و دیپ لرنینگ و تو انداخت این قدر که چیز مهمیه

[150.00s -> 180.00s] حالا به نظر من یکیش دیپ لرنینگه یا همون یادگیری عمیق و یکی دیگه اش مکانیزم توجهه یا همین مدل‌های ترنسفرمری. بزارید یه مثال بزنم، فکر کنم این طوری بهتر جا بیوفته. نت فیلیکس اومد سال دو هزار و شش یه مسابقه راه انداخت و

[180.00s -> 210.00s] مسئله اش هم همون مسئله سیستم های پیشنهاد دهنده بود. ریکامندر سیستم ها. میخواست خودش استفاده کنه دیگه. مسئله اش چی بود؟ مسئله اش این بود که میگفت من یه سری کاربر دارم که این کاربر ها رفتن یه سری فیلم دیدن و امتیاز دادن. پس مشخصاً من میدونم که هر کاربری به چه فیلمی چه امتیازی داده

[210.00s -> 240.00s] میخوام که آیدی این کاربر رو میدم به شما توی دیتابیس آیدی فیلم هم میدم و امتیازش هم بین یک تا پنج میدم و شما زحمت بکشید یاد بگیرید این رو. که من وقتی گفتم که این کاربر به یه فیلم جدید چه نمره ای میده شما بتونید این رو پیش بینی کنی. یعنی کل مسئله این بود که بفهمیم که اون طرف چه فیلمهایی رو دوست داره و خب

[240.00s -> 270.00s] اصلا مسئله ریکامنتی چیز نمیگه مسئله خیلی متداولی هست که وجود داره خلاصه نت فلیکس اومد این رو داد دیگه. این مسئله رو داد و هر کسی اومد از راه حل خودش استفاده کرد که راه حل‌هایی که بودن راه حل‌هایی بودن که مثلا فرض کنید طرف اومد از توابع خطی استفاده کرد برای پیش بینی

[270.00s -> 300.00s] یه مسئله تو فضای سه بعدی یه یوزر یه فیلم و یه رتبه اش

[300.00s -> 330.00s] انتزاعی بشه اگر بخوایم که این مسئله رو با مدل های خطی حل کنیم. عملا داریم تو این فضای سه بعدی یه سری خط می کشیم یا یه سری صفحه

[330.00s -> 360.00s] بعد مثلا میگیم که اونایی که بالای این صفحه اند، این فیلم رو دوست دارن و اونایی که پایین این صفحه اند، این فیلم رو دوست ندارن

[360.00s -> 390.00s] اتوماتیک. نه که خود مون بشینیم تقسیم کنیم خودشون میان این فضا رو به یه سری ناحیه تقسیم میکنن. بعد میگن که اونایی که مثلا تو این ناحیه میوفتن این فیلم رو دوست دارن اونایی که تو فلان ناحیه بیوفتن فلان فیلم رو دوست دارن

[390.00s -> 420.00s] این که میگم اینه که خودشون برن این فضاها رو برا خودشون در بیارن مثلا تو این توابعی که هست بگردن یه تابع پیدا کنن خب تا همین الانش هم فکر می کنم خیلی براتون پیچیده شده باشه ولی برا درک دیپ لرنینگ در نظر بگیرید که این فضای ما مثلا تو این مسئله نت فیلیکس

[420.00s -> 450.00s] کوچیک بود سه بعد بود ولی تو خیلی از مسایلی که داریم این فضامون کوچیک نیست مثلا شما یه عکس ساده یه تصویر ساده اگر با گوشی تون بگیرید خیلی کم باشه مثلا چهار هزار پیکسل در دو هزار پیکسل میشه هشت میلیون پیکسل

[450.00s -> 480.00s] هر کدومش یه آر و جی و بی داره که میشه فضای بیست و چهار میلیون بعدی یعنی حالا اگر بخوایم از یه مدل خطی استفاده کنیم، باید بریم تو این فضای بیست و چهار میلیون بعدی یه سری خط بکشیم یا یه سری صفحه

[480.00s -> 510.00s] یعنی همین الانش هم خیلی پیچیده است. حالا این چیزی که دارم راجع بهش صحبت می کنم ولی این ریاضیدان‌ها خب این جوری بودن که کسایی که داشتن کار میکردن این جوری بودن که ما پیچیده تر از این میخوایم، یعنی میخوایم بتونیم تو اون فضای مثلا صد میلیون بعدی بتونیم

[510.00s -> 540.00s] یه سری توابع نمیدونم کروی تعریف کنیم یه سری توابع خیلی پیچیده تعریف کنیم و اینجا بود که دست شون تو پوست گردو مونده بود و حالا اینجاست که دیپ لرنینگ میاد به کمک دیپ لرنینگ میاد به کمک و میگه آقا من میتونم اینجا به تون کمک کنم تو مسایلی که تو یه سری مسایل خاص

[540.00s -> 570.00s] میتونم به تون کمک کنم که پیچیده تر فکر کنید یعنی از این سادگی در بیاید از اون سادگی که خودش خیلی پیچیده است در بیاید. از این هم برید دوباره انتزاعی‌تر فکر کنید و بعدش می‌بینیم که این ریاضی دارن ول نمیکنن. حالا کم کم با هم می‌ببینیم که اینا هی میرن پیچیده‌تر فکر میکنن. انقدر پیچیده فکر میکنن که دیگه اصلا نمیشه گفت یعنی یه سری جاها

[570.00s -> 600.00s] میبری شون روی فضای مثلا یک میلیون بعدی به شون یه ابزاری میدی که بتونن تو این فضای یه میلیون بعدی کار کنن. بعد طرف میرن مثلا دیفرانسیل رو برمی داره میاره تو این فضا با یه سری چیزای امید ریاضی و آنتروپی و یه سری مفاهیم دیگه هر کی بهش میکنه بعد اصلا دیگه نمیشه فهمید داره چی کار میکنه

[600.00s -> 630.00s] مدل ها میدن جواب خیلی جذابیه مثلا مدل های دیفیوژن خیلی قشنگ دو تا تصویر رو میگیرن با هم ترکیب میکنن یه تصویر بینابینی میدن که خیلی خوشگله یعنی هر کسی ببینه تایید میکنه که این تصویر بینابینی خیلی قشنگه و کار جذابیه ولی آدم ریاضیاتشو میخواد بره بخونه سرش درد میگیره

[630.00s -> 660.00s] خلاصه حرفم اینه که ما خیلی از مسائلو میتونیم با همون توابع حالا میگن یادگیری ماشین کلاسیک بهش میگن خیلی از مسائل دنیای واقعیو میتونیم با همون یادگیری ماشین کلاسیک حل کنیم ولی نیاز به یه جعبه ابزاری داشتیم نیاز به یه سری روشا داشتیم که بتونیم مسائل خیلی پیچیده تریام حل کنیم و

[660.00s -> 690.00s] این جا بود که دیپ لرنینگ به وجود اومد و این اجازه رو به ما داد

[690.00s -> 720.00s] که قبلا بود یا پیاده سازی شده بود به نوعی. اینا رو با هم ترکیب کردن و یه مدل جدید به وجود اومد و باعث شد که اون فضایی که میتونیم توش بگردیم بزرگ شه

[720.00s -> 750.00s] اول اومدن مفهومه تابع تودر تولو اضافه کردن یعنی تو ریاضی یادتونه میگفتن که اف جی ایکس یا مثلا جی اف ایکس داستان این بود که گفتن اگه میخوای پیچیدگی داشته باشی به اندازه کافی بیا این توابع رو به صورت زنجیر به هم وصل کن یعنی اول ورودیتو

[750.00s -> 780.00s] خطی ازش خروجی می گیری این خروجیه رو به یه تابع غیر خطی ازش خروجی بگیر دوباره به یه تابع خطی دوباره به تابع غیر خطی دوباره به تابع خطی همین جور مثلا تا صد مرحله برو جلو بعد اینکه تا صد مرحله میرفت جلو. میگفتن آره شما یه شبکه یه عصبی یه صدلایه درست کردی

[780.00s -> 810.00s] بعد پیچیده هم بود دیگه. خوشحال بودن چون اون وسطای تابع غیر خطی داشتن بینش هم تابع خطی بود. مثلا اینجوری بود که یه خطی میذاشتن یه دونه سینوس یه خطی یه دونه ای توانی یه خطی یه دونه تابع نمیدونم هر تابعی که دوست داشته باشید که غیر خطی باشه. یعنی خط نباشه بعد این فضا خیلی پیچیده میشه دیگه

[810.00s -> 840.00s] از این استفاده کردن و خیلی هم خوشحال بودن و گفتن که خب دست شما درد نکنه اینو چجوری ترین کنیم اینی که میگی خیلی فضای جذابیه. چجوری میتونیم ترینش کنیم بهینه‌سازیش کنیم. اینا اومدن از یه الگوریتم خیلی قدیمی استفاده کردن به نام گرادیان کاهشی. این الگوریتم میگفت که هر تابعی رو بدی به من

[840.00s -> 870.00s] مشتقشم بدی من برات بهینه می کنم. یعنی میرم تو اون فضای توابع برات یه تابع خوب پیدا می کنم فقط مشتقشو بم بده آقا. یعنی اون توابعی که میزاری اون وسط مشتق پذیر باشه. اینجوری نباشه که دست منو بذاری تو پوست گردو. خب مشتقشم بهم بده. تابع هم بهم بده و دیگه حله من میتونم .برات پیدا کنم حالا

[870.00s -> 900.00s] تابع تو درتو اومدن با مشتق ترکیب کردن مشتق زنجیری استفاده کردن. ولی ایناش مهم نیست. اصلا نمیخوام وارد این بحثاش بشم. بحث اینه که از ترکیب یه سری توابع اومدن استفاده کردن برای اینکه اون پیچیدگی که میخوانو به دست بیارن ولی فقط اینم نبود یه چند تا عامل دیگه بود که باعث شد که دیپ لرنینگ پا بگیره

[900.00s -> 930.00s] یکی دیگش حجم عظیم داده ای بود که به وجود اومده بود یعنی تو اینترنت یک عالمه مثلا تصویر به وجود اومده بود یک عالمه آدم محتوا تولید کرده بودن بعد شما میتونستی یه مدلی رو آموزش بدی ازش بخوای که مثلا این تصویرها رو از هم تشخیص بده و بگه که مثلا

[930.00s -> 960.00s] این تصویر سگه یا گربه یا اسبه یا شتره عامل بعدیش هم بحث GPU بود که اومدن گفتن که شما چرا برمی داری از GPU فقط برای بازی استفاده می کنی چون اگر یاد تون باشه حالا من خودم بچگی بودم این جوری بود که جی پی او مثلا جنگ‌های صیلی می خواستم بازی کنم جی پی او یه مقداری خواستم

[960.00s -> 990.00s] از حافظه جی پی او میخواست یا اینکهGTA پی او خیلی زیاد مصرف میکرد و تو بازی خیلی تنها جایی که کاربرد داشت جی پی او برای بازی بود و رندرینگ و از این بحثا ولی تو ترین کردن مدل استفاده نمیشد. جی پی او اینا اومدن گفتن که خب بریم تو

[990.00s -> 1020.00s] ترین کردن مدل هم از جی پی او استفاده کنیم. یعنی چون کاری که داریم انجام میدیم ما تو ترین کردن مدل، یه سری ضرب ماتریسه یه سری جمع ماتریسیه و عملیاتایی که انجام میدیم همش عملیاتای ماتریسیه و اصلا یه زبانم داره جی پی او برای خودش زبانش CUDA هست که با کودا برنامه ها شو مینویسن

[1020.00s -> 1050.00s] اینا رفتن که حالا برای اولین بار از جی پی او استفاده کردن برای این که بتونن مدل شون ترین کنن یعنی از هر کی به این چارت ها استفاده کردن و این طوری شد که تونستن توی جایی تو تشخیص تصویر که حالا اولین چیزی بود که اولین مدلی بود که یه مدل دیپ بود اومدن از همین ترکی به تواب

[1050.00s -> 1080.00s] جی پی او اینها استفاده کردن و یه مدل خیلی خوب ارائه دادن

[1080.00s -> 1110.00s] که بتونی خروجی خوب بگیری

[1110.00s -> 1140.00s] مدل ترین کنن به این رسیدن که اینجور توابعی رو بزاری پشت سر هم خروجی جالب میده

[1140.00s -> 1170.00s] اصلا این مدلی که داریم میسازیم چند لایه عمیقش کنیم و بعد به نتایج جالبی رسیدم یکی از نتایجی که بهش رسیدم این بود که تا یه جایی هرچی این مدل رو عمیق تر کنیم هرچی این مدل تو در تویش بیشتر بشه ظاهرا میتونه به دقت کمک کنه

[1170.00s -> 1200.00s] ولی از یه جایی به بعد بازم به محدودیت محاسباتی می خوردم یعنی تعداد پارامترایی که داشتم این توابع تو در تویی که میذاشتم هر تابع خودش کلی پارامتر داشت تعداد پارامترایی که داشتم وقتی زیاد میشد این دفعه هزینه ترینینگ هم میرفت بالا این دفعه نیاز داشتیم که دیتا سه تا بزرگتری داشته باشیم

[1200.00s -> 1230.00s] که مدلی که به دست میاد مدلی باشه که بهتر بتونه کار کنه

[1230.00s -> 1260.00s] بعد از اینکه دیپ لرنینگ اومد یه جعبه ابزار اومد یه ابزاری درست شد که طرف میگفت که شما یه سری مجموعه توا به بده من که اینارو هر جوری دوست داری تو در توش کن

[1260.00s -> 1290.00s] تا به جایی اختراع کنی وسط هرچی که دوست داری بزار وسط فقط تنها کاری که باید بکنی اینکه زحمت بکشی مشتقه اون توابع بدی به من که خب مشتقه ام خیلی کار سختی نیست یعنی اصلا از یه جایی به بعد آنقدر دیدن که این مدل ها راحته کار کردن با هاش و ترین کردنش که این جوری بود که

[1290.00s -> 1320.00s] یه سری کتابخونه اومد کتابخونه پایتورچ اومد کتابخونه تنسرفلو اومد یعنی شرکت های بزرگ مثل فیسبوک و گوگل رفتن کتابخونه هایی نوشتن که گفتن اگر میخوای مدل ترین کنی بیا از کتابخونه های ما استفاده کن. ما خود مون اتوماتیک مشتقش می گیریم یعنی اینا هم حلش کردن رو GPU هم خود مون می بریم یعنی اصلا نیازی نیست

[1320.00s -> 1350.00s] بری اون زبان کودا رو هم داد چیز کنی خودت یاد بگیری ما خودمون تو پایتون بنویس ما خود مون میریم به کودا تبدیل می کنیم که رو جیپیو اجرا بشه

[1350.00s -> 1380.00s] هر کسی میتونه برابر خودش یه مجموعه توابع فک کنه که چجوری بزاریم که مدله بهتر ترین بشه

[1380.00s -> 1410.00s] که عملا دیگه تابه و اونو بزارم وسط میگن مشکل نداره شما تا زمانی که این توابه ای که داری میزاری این وسط مشتقه شو بتونی بگیری و حالا یا خودت بگیری یا ما اتوماتیک بتونی مشتقه برات بگیریم مسئله ای وجود نداره شما هر تابه ای دوست داری بزاری این وسط من برات بهینه ساز می کنم و خودتون میتونی تصور کنید که

[1410.00s -> 1440.00s] محقق ها با دیدن این چقدر خوشحال شدن و چقدر منابع انرژی وقت و هر چیزی که بود جی پی او رفت سراغ این که بریم حالا ببینیم چطوری از این مدله استفاده کنیم یعنی چطوری از این ظرفیت جدیدی که به دست اومده استفاده کنیم

[1440.00s -> 1470.00s] میتونید تصور کنید از اینجا که خیلی از فیلدهایی که توی هوش مصنوعی داشتیم تغییرات قابل توجهی پیدا کرد

[1470.00s -> 1500.00s] خصوصا جاهایی که داده ای که داشتیم داده غیر ساختار یافته ای بود مثلا صوت داشتیم همین صوتی که من دارم با هاتون صحبت می کنم یا تصویر داشتیم یا متن داشتیم این جاها تغییرات خیلی قابل توجهی داشتیم و روز به روز مدلهایی اومدن که میرفتن توی تسک خاص توی یه چیز خیلی خاص

[1500.00s -> 1530.00s] مثلا تشخیص شی یا دسته بندی شی یا تشخیص احساس توی تسک خیلی خاص خیلی خوب عمل میکردن و شاید دقت شون نزدیک به آدمیزاد میشد تو اون تسک ولی در نظر بگیرید که باز هم داریم راجع به توابع حرف می زنیم و هنوز تو همون فضا هم

[1530.00s -> 1560.00s] روز به روزم منابعی که استفاده می کنیم بیشتر میشه

[1560.00s -> 1590.00s] و میگه که احتمالا تو پنج سال آینده هم همین روند رو هایم داشت و مدل ها مون مثلا میشه هزار برابر نسبت به الان یا ده هزار برابر

[1590.00s -> 1620.00s] به این استفاده کنیم و مدل های خیلی پیچیده ای رو ترین کنیم بعد از این که دیپ لرنینگ این توانایی رو به ما داد بعد از مدتی دوباره یه نقطه عطف اتفاق افتاد و یه تاب جدیدی معرفی شد یعنی محقق های گوگل اومدن یه تابع جدیدی رو معرفی کردن به نام Attention توی مقاله ای به نام اتنشن

[1620.00s -> 1650.00s] توجه تمام اون چیزیا که بهش نیاز داریم. به این تابع گفتن توجه. من اصلا کار به تابعش ندارم ولی از یه جایی به بعد یه تابعی به ما اضافه شد که باعث شد که بتونیم خیلی مدل های بزرگ تری ترین کنیم و خصوصا تو تکس. خصوصا تو متن باعث شد که بتونیم روز به روز مدل های بزرگ تر ترین کنیم

[1650.00s -> 1680.00s] و این طوری که تا الان به نظر میرسه اینه که هرچی مدل ها مون سایزش بزرگ تر بشن توانایی های شناختشونم بهتر میشه توانایی هایی که پیدا می کنم بهتر میشه یعنی نه تنها تسکای ساده رو بهتر میتونن انجام بدن مثل تشخیص احساس و نمیدونم تشخیص اسپم بودن یا نبودن بلکه الان به وضعیتی رسیدن

[1680.00s -> 1710.00s] که ما میریم با ChatGPT صحبت می کنیم و خیلی وقتا جوابهایی به ما میدن که اگر یه آدم میخواست به اون جواب بده این قدر خوب نبود حالا اگر عمر و فرصتی باشه از قسمت بعد میریم راجع به مدل های زبانی بزرگ یا Large Langage Modelها که ChatGPT یه نمونه نشونه و میریم راجع به این ها صحبت می کنیم

[1710.00s -> 1740.00s] امیدوارم که این قسمت برا تون مفید بوده باشه خدا نگه دار

[1740.00s -> 1745.93s] شما باشه خدا نگه دار